# TOP K
### 10亿个数据中找出最大的10000个？——最小堆
最小堆法
	先拿10000个数建堆，然后逐个添加剩余元素；如果大于堆顶的数（10000中最小的），将这个数替换堆顶，并调整结构使之仍然是一个最小堆
	遍历完后，堆中的10000个数就是所需的最大的10000个
	复杂度分析：时间复杂度是O（mlogm），算法的时间复杂度为O（nmlogm）（n为10亿，m为10000）
优化方法
	如果内存受限：可以直接在内存总使用Hash方法将数据划分成npartition，每个partition交给一个线程处理，线程的处理逻辑可以采用最小堆，最后一个线程将结果归并
进一步优化：
	该方法存在一个瓶颈会明显影响效率，即数据倾斜。每个线程的处理速度可能不同，快的线程需要等待慢的线程，最终的处理速度取决于慢的线程。而针对此问题解决的方法是，将数据划分成c×n个partition（c>1），每个线程处理完当前partition后主动取下一个partition继续处理，直到所有数据处理完毕，最后由一个线程进行归并
如果含较多重复值：先用hash / 依图法去重，可大大节省运算量
### 有几台机器存储着几亿淘宝搜索日志，你只有一台 2g 的电脑，怎么选出搜索热度最高的十个？
针对top k类文本问题，通常比较好的方案是【分治+trie树/hash+小顶堆】，先将数据集按照hash方法分解成多个小数据集，然后使用trie树或者hash统计每个小数据集中的query词频，之后用小顶堆求出每个数据集中出频率最高的前K个数，最后在所有top K中求出最终的top K
- 拆分成n多个文件：以首字母区分，不同首字母放在不同文件，长度仍过长的继续按照次首字母进行拆分。这样一来，每个文件的每个数据长度相同且首字母尾字母也相同，就能保证数据被独立的分为了n个文件，且各个文件中不存在关键词的交集
- 分别词频统计：对于每个文件，使用hash或者Trie树进行进行词频统计
- 小顶堆排序：依次处理每个文件，并逐渐更新最大的十个词
### 提取某日访问次数最多的IP地址
将去进行 %1024 取模散列到1024个文件中（划分子文件方法之一）
采用hashmap对其进行次数统计 最后用排序算法进行排序
### 数据太多，内存不够，找到最热门的K个数据
首先对数据进行预处理，采用hash表统计次数 然后再排序
# 海量数据排序、压缩问题
### 重要方法——位图法 Bitmap
位图的基本概念：用一个位（bit）来标记某个数据的存放状态。例如，有{2, 4, 5, 6, 67, 5}这么几个整数，维护一个 00…0000（共67位）的0/1字符串，1表示该索引（=数据值）处存在数，0则表示不存在
应用：位图法可以用于海量数据排序，去重，压缩
优点：针对于稠密数据集可以很好体现出位图法的优势，内存消耗少，速度较快
缺点：不适用于稀疏数据集，比如我们有一个长度为10的序列，最大值为20亿，则构造位串的内存消耗将相当大250M，而实际却只需要40个字节，此外位图法还存在可读性差等缺点
### 非重复排序
假设有一个不重复的整型序列{n1， n2， ... ,nn}，假设最大值为nmax，则可以维护一个长度为nmax的位串。主要过程就是2步：
- 第一遍遍历整个序列，将出现的数字在位串（java中可以用数组实现）中对应的位置置为1；
- 第二遍遍历位图，依次输出值为1的位对应的数字，这些1所在的位串中的位置的索引代表序列数据，1出现的先后位置则代表序列的大小关系
### 数据压缩
**前提**：数据中存在大量的冗余值  
基本思路就是使用某个子串存储原数据中的海量值
### 如何用redis存储统计1亿用户一年的登陆情况，并快速检索任意时间窗口内的活跃用户数量
在redis 2.2.0版本之后，新增了一个位图数据。redis单独对bitmap提供了一套命令。可以对任意一位进行设置和读取。所以可以在位图中使用1表示活跃。
bitmap的核心命令：
- SETBIT：设置某位为1，语法：`SETBIT key offset value`
- GETBIT：获取某位的值  语法：`GETBIT key offset`
bitmap的其他命令还有bitcount，bitpos，bitop等命令。都是对位的操作。
- 获取某一天id为88000的用户是否活跃：getbit 2020-01-01 88000 [时间复杂度为O(1)]
- 统计某一天的所有的活跃用户数：bitcount 2019-01-01 [时间复杂度为O(N)]
- 统计某一段时间内的活跃用户数，需要用到bitop命令。这个命令提供四种位运算，AND(与)，(OR)或，XOR(亦或)，NOT(非)。我们可以对某一段时间内的所有key进行OR(或)操作，或操作出来的位图是0的就代表这段时间内一次都没有登陆的用户。那只要我们求出1的个数就可以了
### 海量文本去重——simhash法
[参考](https://cloud.tencent.com/developer/article/1379302?from=14588)
# 资源 vs 请求问题
### 两百个骑手抢一个单子
1. 单机使用采用volatile关键字修饰该订单采用CAS操作对其进行乐观锁操作
2. 采用redis，zookeeper分布式锁加锁
3. 消息队列 实现幂等接口
# 怎么实现朋友圈的发和查看
微信朋友圈 一个用户好多好友
发朋友圈-自己能看到，朋友也能看到
用户量比较大，活跃用户比较多，用户好友数有限制，发朋友圈数量有限制
# 设计一个消息队列
### 整体思路
**1.Producer(消息生产者)**：发送消息到Broker
**2.Broker(服务端)**：Broker这个概念主要来自于Apache的ActiveMQ，特指消息队列的服务端。主要功能：把消息从发送端传送到接收端，这里会涉及到消息的存储、消息通讯机制等
**3.Consumer(消息消费者)**：从消息队列接收消息，consumer回复消费确认
### Broke设计重点
1. 消息转储，保证合适时间投递，通过辅助手段保证消息抵达消费者
2. 规范范式，保证消息解耦、错峰等
3. 消息转发器，实现两次RPC，即P->B，B->C
### 通信协议
JMS：
提供了两个功能
1）点对点
2）以及publish-subscribe（发布订阅）模型，无需感知消费者存在
AMQP：
AMQP不是一个具体的消息队列实现，而 是一个标准化的消息中间件协议。
目标是让不同语言，不同系统的应用互相通信，并提供一个简单统一的模型和编程接口。 目前主流的ActiveMQ和RabbitMQ都支持AMQP协议。
AMQP是一种协议，更准确的说是一种binary wire-level protocol（链接协议）。这是其和JMS的本质差别，AMQP不从API层进行限定，而是直接定义网络交换的数据格式
# 存储选型
对于分布式系统，存储的选择有以下几种
1）内存
2）本地文件系统
3）分布式文件系统
4）nosql
5）DB
从速度上内存显然是最快的，对于允许消息丢失，消息堆积能力要求不高的场景(例如日志)，内存会是比较好的选择
DB则是最简单的实现可靠存储的方案，很适合用在可靠性要求很高，最终一致性的场景(例如交易消息)，对于不需要100%保证数据完整性的场景，要求性能和消息堆积的场景，hbase也是一个很好的选择
理论上，从速度来看，文件系统>分布式KV（持久化）>分布式文件系统>数据库，可靠性却截然相反。
还是要从支持的业务场景出发作出最合理的选择，如果你们的消息队列是用来支持支付/交易等对可靠性要求非常高，但对性能和量的要求没有这么高，而且没有时间精力专门做文件存储系统的研究，DB是最好的选择
对于不需要100%保证数据完整性的场景，要求性能和消息堆积的场景，hbase也是一个很好的选择，典型的比如 kafka的消息落地可以使用hadoop
# 消费关系处理
抛开现象看本质，无外乎是**单播与广播**的区别。
所谓单播，就是点到点；而广播，是一点对多点。
为了实现广播功能，我们必须要维护消费关系，通常消息队列本身不维护消费订阅关系，可以利用zookeeper等成熟的系统维护消费关系，在消费关系发生变化时下发通知
# 如何把一个文件快速下发到 100w 个服务器
采用p2p网络形式，比如树状形式，网状形式，单个节点既可以从其他节点接收服务又可以其他节点提供服务
对于树状传递，在100W台服务器这种量级上，可能存在两个问题：
如果树上的某一个节点坏掉了，那么从这个节点往下的所有服务器全部宕机。
如果树中的某条路径，传递时间太长了（网络中，两个节点间的传递速度受很多因素的影响，可能相差成百上千倍），使得传递效率退化。
改进：100W台服务器相当于有100W个节点的连通图。那么我们可以在图里生成多颗不同的生成树，在进行数据下发时，同时按照多颗不同的树去传递数据。这样就可以避免某个中间节点宕机，影响到后续的节点。同时这种传递方法实际上是一种依据时间的广度优先遍历，可以避免某条路径过长造成的效率低下

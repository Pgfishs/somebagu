# cpu是如何执行程序的
### 冯诺依曼模型
五个部分**运算器、控制器、储存器、输入设备、输出设备**。
运算器、控制器在中央处理器中，储存器就是常见的内存，输入输出设备是计算机外接的设备。储存单元和输入输出设备要与中央处理器打交道，离不开总线
#### 内存
储存的区域是线性的，储存数据的基本单位是**字节***byte*，一字节等于八位（8bit），每一个字节对应一个内存地址，从0开始
#### 中央处理器
32位cpu一次可以处理4个字节，64位一次可以处理8个字节，32位和64位代表CPU的位宽，位宽越大，可以计算的数值就越大。
cpu内部还有一些组件，常见的有**寄存器、控制单元、逻辑运算单元**。其中控制单元负责控制CPU工作，逻辑单元负责计算，寄存器又可以分为多种类，每种功能不同，紧挨着控制单位和逻辑运算单元
**常见寄存器种类**
* *通用寄存器*，用来存放需要运算的数据
- *程序计数器*，储存cpu要执行的下一条指令**所在的内存地址**
- *指令寄存器*，用来存放正在执行的指令
#### 总线
用于cpu和内存和其他设备之间的通信，可分为三种
- **地址总线**，用于指定CPU将要操作的内存地址
- **数据总线**，用于读写内存数据
- **控制总线**，用于发送和接收信号
CPU读写内存时，依次经过**地址总线-控制总线-数据总线**
### 程序执行的基本过程
CPU根据程序计数器里的内存地址，从内存里面把需要执行的指令读取到指令寄存器中执行，然后指令长度自增，开始顺序读取下一条指令，这个循环过程叫**CPU的指令周期**
### a = 1 + 2执行过程
- 编译器先将数据1和数据2放到**数据段**中
- 编译器将 a=1+2编译为四个指令
  > load指令将地址1中的数据1装入寄存器R0
  > load指令将地址2中的数据2装入寄存器R1
  > add将R0 R1数据相加放入寄存器R2
  > store指令将R2数据存回数据a对应的地址中
- 编译完后，运行程序时程序计数器将地址设置为0x100地址，然后依次执行这4条指令
#### 指令
MIPS指令是32位的整数，高6位代表着操作码，剩下26位不同指令类型所表示内容也不同，主要有R、I、J三种
- **R指令**，用在算数和逻辑操作，有读取和写入数据的内存地址
- **I指令**，用在数据传输、条件分支等
- **J指令**，用在跳转，高6位之外的26位都是一个跳转后的地址
CPU构造指令流水线通常被分为四个阶段
1. Fetch，CPU通过程序计数器读取对应内存地址的指令
2. Decode，CPU对指令进行解码
3. Execution，CPU执行指令
4. Store，CPU将计算结果存回寄存器或将寄存器值存入内存
#### 指令类型
- 数据传输类型的指令
- 运算类型的指令
- 跳转类型的指令
- 信号类型的指令
- 闲置类型的指令
# 磁盘比内存慢几万倍？
### 储存器的层次结构
- 寄存器
- CPU Cache
  >1. L1 cache
  >2. L2 cache
  >3. L3 cache
- 内存
- SSD/HDD硬盘
#### 寄存器
最靠近CPU控制单元和逻辑单元的储存器，32位CPU中大多数寄存器可以储存**4个字节**，64位大多数可以储存**8个字节**
#### CPU Cache
SRAM静态随机储存器
**L1高速缓存**：通常缓存**指令缓存和数据缓存**
**L2高速缓存**：离核心更远，访问速度10-20时钟周期
**L3高速缓存**：多个CPU公用，访问速度20-60时钟周期
#### 内存
DRAM动态随机存取储存器，访问速度200~300时钟周期
#### SSD/HDD
内存读写速度是SSD的10-1000倍，HDD的10w倍
### 储存器的层次关系
# 如何写出让CPU跑得更快的代码
### CPU Cache的数据结构和读取过程是什么
CPU Cache的数据是从内存中读过来的，一小块一小块的**Cache Line**，CPU会顺序Cache Line中的数据。
CPU读取数据的时候，无论数据是否在Cache中，都是先访问Cache，没有数据时再去访问内存，加载到cache中
内存的访问地址，包括**组标记、CPU Cache Line、偏移量**组成，CPU通过这些信息，就能在CPU Cache中找到缓存的数据。对于CPU Cache中的数据结构，**则是引索+有效位+组标记+数据块**组成
### 如何写出让CPU跑的更快的代码
数据在CPU Cache中，意味着**缓存命中**，缓存命中率越高，代码性能就会更好
L1通常分为**数据缓存、指令缓存**，分开来看这两个的缓存命中率
#### 如何提升数据缓存的命中率
遇到遍历数组情况时，按照内存布局顺序访问，可以有效的利用CPU Cache带来的好处，提升代码性能
#### 如何提升指令缓存的命中率
**CPU分支预测器**可以分支预测接下来需要执行的的if/else里的数据，可以提前将指令放在指令缓存里，这样CPU可以直接从Cache读取到指令，执行速度就很快
#### 如何提升多核CPU的缓存命中率
如果一个线程在多个核心中切换，各个核心的命中率就会受到影响，如果线程都在一个核心上执行，其数据的L1和L2的缓存击中率就可以有效提高。当多个同时执行计算密集型的线程，可以把线程绑定在一个CPU核心上
# CPU缓存一致性
### CPU Cache的数据写入
#### 写直达
把数据同时写入Cache和内存中，很大影响性能
如果数据**已经**在Cache中，则先更新到Cache里，再写入内存；**没有**在Cache中，则直接把数据更新到内存里
#### 写回
当发生写操作时，新数据只写入Cache Block中，当修改过的Cache Block被替换时才写入内存
**先修改Cache中的数据**，然后通过脏位判断是否被修改过；若修改过写回主存，没修改过不必写回
CPU----->Cache------>主存
#### 缓存一致性问题
多个核心的L1，L2是独有的，一个核心操作了变量在L1中而没有写入内存，其他核心就会读取到错误的变量
- 当某个核心Cache的数据更新时，必须传播到其他Cache，被称为**写传播**
- 某个核心对数据的操作顺序，必须在其他核心看起来顺序一样，称为**事务串行化**
要实现事务串行化，要做到两点
- cpu核心对于cached数据的操作，必须同步给其他核心
- 要引入*锁*的概念，如果两个CPU核心有相同数据的cache，对于这个cache数据的更新，只有拿到了锁，才能进行对应的数据更新
### 总线嗅探
写传播的原则就是当某个CPU核心更新了cache的数据，要把该事件广播到其他核心，最常用的方法是**总线嗅探**
当核心A修改了L1中的数据，通过总线将这个时间广播给其他核心，其他核心都监听总线上的广播，检测并更新已有数据。CPU需要每时每刻监听总线上的一切活动，回加重总线负担
总线嗅探只保证了某个核心的cache更新这个事件被其他核心知道，不能保证事务串行化
### MESI协议
- Modified 已修改
- Exclusive 独占
- Shared 共享
- Invalidated 已失效
这四个状态标记cache line的四个不同的状态
**已修改**就是脏标记，代表该cache block中的数据已经被修改过了，但还没有写入内存；**已失效**代表该cache block中的数据已经失效，不可再读取
**独占、共享**状态都标识cache block中的数据是干净的，即内存和cache数据一致；**独占**状态数据只储存在一个核心的cache中，其他核心cache没有数据，该核心可以任意修改数据，当独占状态下的数据，如果有其他核心cache从内存中读取到，则会变为**共享**；共享状态下不能直接修改数据，而是要先向其他核心广播一个请求，将其他cache的cache line修改为*无效*状态，然后再更新数据
# CPU是如何执行任务的
### CPU如何读写数据
Cache<-Memory<-SSD/HHD
CPU Cache Line是CPU从内存读数据到Cache的，连续读取的一块一块的数据
#### 分析伪共享
多个线程同时读写一个Cache Line的不同变量时，导致CPU Cache失效的现象称之为**伪共享*false sharing***
#### 避免伪共享的方法
对于多个线程共享的热点数据，应当避免这些数据在同一个cache line中
Linux内核中存在`__cacheline_aligned_in_smp`宏定义，用于解决伪共享问题
- 多核系统中，该宏定义为cache line的大小
- 单核系统该宏定义是空的
为了防止伪共享现象，可以使用上面的宏定义让变量在cache line中对齐，使用空间换时间方法提升性能
或前置、后置填充cache line，使得整个cache line怎么加载都没有发生更新操作的数据，只要数据被频繁的读取访问，就没有数据被换出cache line，也不会产生伪共享的问题
### CPU如何选择线程的
linux内核中，进程和线程都是用`task_struct`结构体表示的，进程的task_struct里部分资源是共享了进程已创建的资源，比如内存空间地址、代码段等。没有创建线程的进程，只有单个执行流，被称为主线程，可以创建多个线程分别处理不同事情，对应到内核中都是`task_struct`
linux中任务分为两种，优先级数值越小，优先度越高
- 实时任务，对于系统响应时间要求很高，0-99
- 普通任务，响应时间没有很高的要求，100-139
#### 调度类
Deadline Deadline调度器 SCHED_DEADLINE dl_rq
Realtime RT调度器 SCHED_FIFO SCHED_RR rt_rq
Fair CFS调度器 SCHED_NORMAL SCHED_BATCH cfs_rq
Deadline和Realtime这两个调度类，都是应用于实时任务，共有三个调度策略
- SCHED_DEADLINE 按照deadline调度，距离当前时间点最近的deadline的任务会被优先执行
- SCHED_FIFO 对于相同优先级任务，按照FIFO原则，但是优先级更高的任务可以抢占低优先级的任务
- SCHED_RR 对于相同优先级的任务轮流运行，每个人物都有一定时间片，用完时间片的任务会被放到队列尾部，保证公平性，但是高优先级的任务可以抢占
Fair调度类用于普通任务，由CFS调度器管理
- SCHED_NORMAL 普通任务的调度策略
- SCHED_BATCH 后台任务的调度策略，不和终端进行交互，不影响其他需要交互的任务，可以适当降低优先级
#### 完全公平调度
给每个任务分配**vruntime**，CFS算法调度时，优先选择vruntime少的任务，一个任务运行时vruntime会增加
nice级别越低，任务权重值越大，高权重任务的vruntime少，CFS优先执行
#### CPU运行队列
每个cpu都有自己的运行队列RunQueue，用于描述此CPU上所运行的所有进程。
CFS运行队列cfs_rq是按照红黑树描述，按vruntime大小来描述的
调度类优先级Deadline>Realtime>Fair，Linux选择下一个任务执行的时候，先从dl_rq，然后rt_rq，最后cfs_rq，实时任务总是比普通任务先执行
#### 调整优先级
启动任务时没有指定调整优先级的话，默认都是普通任务Fair由CFS调度器管理，如果想要某个任务有更多执行时间，可以调整nice值，让优先值高的任务执行久一点
nice值是优先级的修正指数，priority(new) = priority(old) + nice，内核中nice范围值0-139，普通任务0-99，优先任务是100-139
不管怎么修改nice值，都是普通任务，可以修改任务优先级和调度策略，使得变成实时任务
# 什么是软中断
### 中断是什么
系统收到硬件的中断请求，会打断正在执行的过程，调用内核中的中断处理程序来响应请求。中断是一种异步的事件处理机制，可以提高系统的并发处理机制
中断请求的响应程序，也就是中断处理程序，要尽可能快的执行完，这样可以减少对正常进程运行调度的影响
中断处理程序在响应中断时，还可能会临时关闭中断，意味着如果当前中断程序中没有执行完之前，系统中其他的中断请求都无法被响应，中断有可能会丢失
### 什么是软中断
Linux为了解决中断处理程序执行过长和中断丢失的问题，将中断过程分成了两个阶段，分别是**上半部和下半部**
- 上半部用来处理快速中断，一般会暂时关闭中断请求，主要负责处理和硬件相关或时间敏感的事情
- 下半部用来延迟处理上半部未完成的工作，一般为内核线程的方式运行
中断处理程序的上部分和下部分可以理解为
- 上半部直接处理硬件请求，硬中断，负责耗时短的工作，特点是快速执行
- 下半部分由内核触发，软中断，负责上半部未完成的工作，耗时较长，特点延迟执行
硬中断是打断CPU执行的任务，然后立刻执行中断处理程序，而软中断是由内核线程的方式执行的，每个CPU都有一个软中断内核线程
# 为什么0.1+0.2 ！= 0.3？
### 为什么负数要用补码表示
十进制转二进制采用除2取余法
补码就是把正数的二进制全部取反再+1
如果负数不是补码形式表示，那么再作基本加减法运算时候还需要判断是否为负数再反转，采用了补码则和正数加减法操作一致
### 十进制小数和二进制的转换
乘二取整法
使用二进制无法精确表示0.1，只能用近似值，就会造成精度损失
### 计算机是怎么存小数的
浮点数
- 符号位：表示数字正负，0正1负
- 指数位：指定小数点在数据中位置，指数可以为正数也可以为负数，指数位越长数据表示范围就越大
- 尾数位：小数点右侧数字，位数长度决定了数的精度
### 0.1 + 0.2 = 0.3吗
不等于
0.1和0.2是两个无限循环小数，计算机只能表示近似值，相加并不等于完整的二进制的0.3
# Linux内核 vs Windows内核
### 内核
内核作为应用链接硬件的桥梁，应用只需关系与内核交互，不用关心硬件的细节
内核有哪些能力
- 管理线程、进程，决定哪个线程、进程使用CPU，**进程调度**
- 管理内存，决定内存的分配和回收，**内存管理**
- 管理硬件，为进程和硬件之间提供通信能力，**硬件通信**
- 提供系统调用，程序和操作系统之间的接口
内核是怎么工作
- 内核空间，只有内核程序可以访问
- 用户空间，专门给应用程序使用
当程序使用用户空间时，常说该程序在**用户态**执行，使用内核空间时，程序则在**内核态**执行
### Linux的设计
Linux设计内核理念主要有几点
- Multitask 多任务
- SMP 对称多处理
- ELF 可执行文件连接格式
- Monolithic kernal 宏内核
#### Multitask
代表Linux是个多任务的操作系统
- 对于单核CPU，可以让每个任务执行一小段时间，时间到了就切换到另一个任务，宏观角度看这是一段时间内执行了多个任务，称之为并发
- 对于多核CPU，多个任务可以被不同核心CPU同时执行，称之为并行
#### SMP
对称多处理，代表每个CPU的地位、对资源使用权限相同，多个CPU共享一个内存，每个CPU都可以访问完整的内存和硬件资源
#### ELF
可执行文件连接格式
ELF将文件分成一个个分段，每个段都有自己的作用
ELF文件通过 编译器->汇编器->连接器 形成执行文件ELF
通过 装载器 将ELF装载到内存中
#### Monolithic Kernal
Linux的内核是个完整的可执行程序，有最高权限，宏内核特征是系统内核的所有模块，比如内存管理、进程调度、文件系统都运行在内核态
与宏内核相反的是微内核，只保留最基本的能力，比如进程调度、虚拟机内存等，将一些应用放到了用户空间。这样服务和服务之间是隔离的，提高了系统的稳定性可靠性
### Windows设计
windows内核是混合型内核，Windows的可执行文件叫PE，**可移植执行文件**
# 为什么要有虚拟内存
### 虚拟内存
进程把使用的地址隔离开来，让操作系统为每个进程分配一套独立的虚拟地址。操作系统会提供一种机制，将不同进程的虚拟地址和不同内存的物理地址映射起来
- 程序使用的内存地址叫**虚拟内存地址**
- 实际存放在硬件里面的地址叫物理内存地址
操作系统通过MMU映射关系将虚拟地址转换为物理地址，通过**内存分段**和**内存分页**来管理虚拟地址和物理地址
### 内存分段
分段机制下虚拟地址由**段选择因子**和**段内偏移量**构成
- 段选择因子就保存在段寄存器里，段寄存器里最重要的是**段号**，用作段表的引索。**段表**里保存的是这个**段的基地址、段的界限和特权等级等**
- **段内偏移量**应当处于0和段界限之间，如果段内偏移量是合法的，就将段基地址加上段内偏移量得到物理物理内存地址
内存分段存在不足之处
- 内存碎片
- 内存交换效率低
#### 内存分段会出现内存碎片吗
内存分段管理可以做到根据实际需求分配内存，**不会出现内部内存碎片**，但是每个段长度不固定，会出现**外部内存碎片**，通过内存交换解决（linux的swap）
#### 为什么分段会导致内存交换效率低
多进程系统分段产生大量外部内存碎片，需要内存交换的时候，要把大片连续内存写进硬盘，而硬盘读访问速度慢得多，如果内存交换的时候是交换的是占内存空间很大的程序，这样整个机器就会显得卡顿
### 内存分页
分页是把整个虚拟内存和物理内存切成一段段固定尺寸的大小。这样一个连续且尺寸固定的内存空间，叫做页
虚拟地址和物理地址之间通过页表来映射，页表保存在内存里，当进程访问的虚拟地址在页表中查不到时，系统会产生一个**缺页异常**，进入系统内核空间分配物理内存、更新进程页表，最后返回用户空间，恢复进程的运行
#### 分页是怎么解决分段的外部内存和内存交换效率低的问题
内存分页采用预分配空间，页与页之间紧密排列没有内存空隙，**但是分页最小单位是一页，会造成内部内存碎片**
如果内存空间不够，会将最近没有使用的内存页面换出（swap out 写入硬盘），需要时再写入（swap in），一次写入磁盘的也只有少数几个页，提高了交换效率
分页使得加载程序时不用一次全部加载进物理内存，完全可以在进行虚拟内存映射后，只有在程序运行中需要用到对应虚拟内存里的指令和数据时，再加载到物理内存中
#### 分页机制下，虚拟地址和物理地址是怎么映射的
虚拟地址地址分为**页号**和**页偏移**，页号作为页表的引索，页表包含物理页每页所在的物理内存的及地址，基地址和页内偏移的组合就形成了物理内存地址
#### 简单分页有什么缺陷吗
页表空间占用大，运行进程越多，占用越大
#### 多级页表 
对于32位系统，将页表的单机页表再分页，将页表（一级页表）分为1024个页表（二级页表），每个页表中包含1024个页表项，形成二级分页
使用了二级分页，一级页表就可以覆盖整个4GB虚拟内存空间，但如果某个一级页表的页表项没有被用到，也就不需要创建二级页表了
对于64位系统，分为四级目录
- 全局页目录项PGD
- 上层页目录项PUD
- 中间页目录项PMD
- 页表项PTE
#### TLB
在CPU中放入一个专门存放最常访问的页表项的Cache，这个Cache就是TLB，通常被称为页表缓存、转址旁路缓存、快表等
### 段页式内存管理
内存分段和内存分页组合起来使用就称之为段页式内存管理
- 先将程序划分为多个有逻辑意义的段
- 再把每个段分为多个页
这样地址结构就由**段号、段内页和页内偏移**三个部分组成
### Linux内存布局
Linux主要采用了分页管理，但是无法避免分段管理，Linux把所有段基地址设为0，也就意味着所有程序的地址空间都是线性地址空间（虚拟地址），段只用于访问控制和内存保护
Linux中虚拟空间分为**用户态**和**内核态**，用户态的分布：代码段、全局变量、BSS、函数栈、堆内存、映射区
# malloc是如何分配内存的
### Linux进程的内存分布长什么样
虚拟地址空间的内部被分为**内核空间**和**用户空间**，每个进程都有独立的虚拟内存，但是每个虚拟内存的内核地址关联的都是相同的物理内存
### malloc是如何分配内存的
- 方式一：通过brk()系统调用从堆分配空间
- 方式二：通过mmap()系统调用在文件映射区分配内存
方式一通过brk()将**堆顶**指针向高地址移动，获得新的内存空间
方式二通过mmap()系统调用中**私有匿名映射**的方式，在文件映射区分配一块内存
- 用户分配内存小于128KB使用brk()
- 大于128KB使用mmap()
### malloc()分配的是物理内存吗
分配的是虚拟内存
如果分配后的虚拟内存没有被访问，虚拟内存是不会映射到物理内存的，只有访问已分配的虚拟地址空间的时候，OS通过查找页表，发现虚拟内存对应的页不在物理内存中，就会触发页中断，然后操作系统会建立虚拟内存和物理内存之间的映射关系
### malloc(1)会分配多大的虚拟内存
malloc在分配内存的时候，会预分配更大的空间作为内存池，malloc(1)时会分配132K字节内存
### free释放内存，会归还给操作系统吗
通过**brk()** 申请的内存，free时不会还给操作系统，缓存在malloc的内存池中，待下次使用
通过**mmap()** 申请的内存，free时会把内存归还给操作系统，内存得到释放
### 为什么不全部使用mmap分配内存
频繁通过mmap分配内存，每次都会发生运行态的切换，还会发生缺页中断（第一次访问虚拟地址后），导致CPU消耗较大
brk()申请内存，由于堆空间是连续的，预分配更大内存在内存池中，下次申请时候，直接从内存池中取出内存块就行，减少了系统调用次数，页减少了缺页中断的次数
### 为什么不全使用brk分配内存
连续申请大于内存池的内存空间，会导致空闲内存碎片的产生，对于小块内存，堆内将产生越来越多不可用的碎片，导致内存泄漏
### free怎么知道释放内存的大小
malloc返回给用户态的内存起始地点比进程堆空间起始地址多了16字节，这16字节保存了内存块的大小，执行free时就会对传入的字节偏移16字节，分析内存块大小
# 内存满了会发生什么
### 虚拟内存有什么作用
- 虚拟内存可以使得进程对运行内存超过物理内存大小，程序运行符合局部性原理，对于没有经常被访问到的内存可以存进硬盘的swap区域
- 每个进程有自己的页表，虚拟空间是独立的，解决了多进程之间地址冲突的问题
- 页表项除了物理地址还有标记属性的比特，比如控制一个页的读取权限，在内存访问方面操作系统提供了更好的安全性
### 内存分配的过程是什么
内存通过malloc申请内存，CPU访问这块虚拟内存时，发现没有映射到物理内存中，CPU就会产生**缺页中断**，进程从用户态切换到内核态，并交给内核*缺页中断函数*处理；缺页中断处理会看是否有空闲内存，没有内核就开始**回收内存**
- 后台内存回收kswapd：物理内存紧张的时候，会唤醒kswapd内核线程回收内存，这个回收的进程是**异步**的，不会阻塞进程执行
- 直接内存回收direct reclaim：如果后台异步回收跟不上进程内存申请的速度，就会开始直接回收，这个回收的过程是**同步**的，会阻塞进程的执行
直接回收内存后，空闲的物理内存还是无法满足物理内存的申请，那么内核就会触发OOM机制
OOM Killer会根据算法选择一个占用物理内存较高的进程kill，释放内存，kill到内存空间够为止
### 哪些内存可以修改
- 文件页：内核缓存的磁盘数据和文件数据。**回收干净页的方式是直接释放内存，回收脏页方式是先写回内存再释放内存**
- 匿名页：这部分内存没有实际载体，比如堆、栈，不能直接回收，**而是通过Swap回收**，先把不常访问的写进磁盘，要用的时候再读入内存
都是基于LRU算法，维护active和inactive两个链表
### 回收内存带来的性能影响
回收方式
- 后台内存回收，kswaped内核线程异步回收，不会阻塞进程
- 直接内存回收，同步回收会阻塞进程，CPU利用率会提高引起系统负荷增高
回收类型
- 文件页：对于干净页，直接释放内存，不会影响性能；脏页需要进行磁盘I/O，会影响性能
- 匿名页：开启了swap机制，会将不常访问的匿名页在磁盘和内存置换，影响性能
解决方式：
#### 调整文件页和匿名页的回收倾向
/proc/sys/ym/swappiness选项调整文件页和匿名页的回收倾向，数值越小越倾向于使用swap回收文件页
#### 尽早触发kswapd异步回收内存
内核定义了三个内存阈值，衡量内存是否充裕
- 页最小阈值(pages_min)
- 页低阈值(pages_low)
- 页高阈值(pages_high)
当剩余内存再页低阈值和最小阈值之间，kswapd0会执行内存回收，直到剩余内存大于高阈值位置
当剩余内存小于最小阈值，就会触发**直接内存回收**
页低阈值可通过内核/proc/sys/vm/min_free_kbytes间接设置，页高阈值和页低阈值都是根据最小阈值计算生成的
增大了min_free_kbytes后，会使得系统预留过多的空闲内存，极端情况下设置的内存接近实际物理内存大小，留给应用程序的内存就会太少导致OOM频繁
如果应用程序关注延迟，就增大min_free_kbytes，关注内存使用量就调小min_free_kbytes
#### NUMA架构下的内存回收策略
SMP指**多个CPU共享资源的电脑硬件架构**，每个CPU对操作资源的权限相等，随着CPU核数增多，总线的带宽压力会越来越大。通过NUMA结构解决SMP的这个问题
NUMA将每个CPU进行分组Node，一个Node可以有多个CPU，每个Node有独立资源，包括内存、IO等，每个Node可以通过互联模块总线通信，但是访问远端Node的内存比访问本地内存耗时多
NUMA架构下，当某个Node内存不足时，系统可以从其他Node寻找内存，也可以在本地内存回收内存
通过/proc/sys/vm/zone_reclaim_mode控制回收方式
### 如何保护一个进程不被OOM杀掉
内核根据oom_badness()对进程打分，分最高的进程首先被杀掉；得分收到两个方面影响
- 进程已使用的物理页面数
- 每个进程的OOM校准值
每个进程的得分默认值都是0，可以通过调整oom_score_adj的数值调整
- 不想某个进程先被杀掉，调整adj，降低被OOM杀掉的概率
- 某个进程无论如何都不能被杀掉，将adj配置为-1000
不建议将业务程序设置为-1000，一旦发生内存泄漏，业务程序就不能被杀掉，OOMkiller就会把其他进程一个个杀掉
# 在4G物理内存的机器上申请8G内存会发生什么
### 操作系统虚拟内存大小
malloc申请虚拟内存，检查到虚拟内存没有映射到物理内存，就会触发缺页中断，缺页中断去查早是否有物理内存，决定是否回收
#### 32位系统场景
32位系统最多只能申请3GB大小虚拟空间，线程申请8GB内存会直接申请失败
#### 64位系统场景
不读写虚拟内存的话，是不会分配物理内存的，可以申请8GB虚拟内存
内核/proc/sys/vm/overcommit_memory参数影响申请内存
- 值为0，允许overcommit，但是过于明目张胆的ovecommit会被拒绝，内核通过算法猜测内存申请是否合理，大概为单次申请不超过free memory+free swap+page cahe+SLAB可回收大小，超过就会拒绝overcommit
- 1，允许overcommit，来者不拒
- 0，拒绝overcommit
#### 设置为1后就可以申请接近128T的虚拟内存了吗
不一定，看服务器物理内存大小，会触发OOM Kill进程。申请虚拟内存的过程中物理内存使用量一直增长，达到直接内存回收就会触发OOM
开启swap就可以申请接近128T虚拟内存
### Swap机制作用
线程申请物理内存超过空闲物理内存大小，开启swap就可以正常申请，否则直接OOM。swap机制是将物理内存中长时间没有操作的内存数据临时保存到磁盘中，运行时再写入内存；当内存使用存在压力时，会触发内存回收，将不常访问的内存数据写入磁盘然后释放内存，再访问时写入内存，这种机制就是swap
- 换出Swap out，将进程暂时不用的数据写入磁盘，释放占用的内存
- 换入Swap in，进程再次使用内存时，将数据从磁盘写入内存中
Linux的swap会在内存不足和内存闲置的时候触发
- 内存不足：需要的内存超过可用物理内存时，内核将内存中不常使用的内存页交换到磁盘上，强制的直接内存回收，会阻塞当前申请内存的进程
- 内存闲置：KSwapd守护进程，负责页面置换，会在空闲内存低于一定水位时，回收内存页中的空闲内存保证其他内存可以申请到内存，KSwapd是后台进程，异步执行不会阻塞当前申请内存的进程
#### Swap的是什么类型的内存
内核缓存的文件数据都有对应的磁盘文件，回收的时候直接写回到对应文件
堆、栈数据等匿名页，不能直接释放，保存进swap分区
# 如何避免预读失效和缓存污染
- 预读失效导致缓存命中率下降
- 缓存污染导致缓存命中率下降
Redis缓存淘汰机制通过实现LFU避免缓存污染；Mysql和Linux是通过改进LRU避免预读失效和缓存污染
### Linux的缓存
缓存在Page Cache中，属于内存空间里的数据
### Mysql的缓存
存在磁盘中，Innodb设计了缓冲池，属于内存空间的数据
- 读取数据时，先读取缓冲池的数据，不存在于缓冲池中再去磁盘中读取
- 修改数据时，先修改缓冲池，然后设置为脏页，再由后台线程将脏页写入磁盘
### 传统LRU如何管理内存
- 当访问的页在内存里，就把该页对应的LRU节点移动到链表头部
- 访问的页不在内存里，除了把该页放入到LRU头部，还要淘汰LRU末尾的页
### 预读失效，怎么办？
#### 什么是预读
程序只想读取0-3KB的数据，磁盘读写基本单位是block4KB，操作系统就读取0-4KB内容，在一个page中装下，同时再读取剩下的4-16KB数据，额外申请3page，减少了磁盘IO次数
#### 预读失效会带来什么问题
提前加载进来的page没有被访问，就是**预读失效**，传统的LRU会把预读页放到LRU头部，当内存不够时候，就会把末尾页淘汰掉，如果一直不被读取到，就会导致不会被访问的预读页占据了LRU前排位置，末尾淘汰的页却可能是热点数据，**大大降低了缓存命中率**
#### 如何避免预读失效造成的影响
让预读页停留在内存中的时间尽可能短，让真正被访问的页移到LRU头部，保证真正被读取的热数据留在内存里的时间尽可能长
- Linux实现了两个LRU，active_list和inactive_list
- Mysql的InnoDB在一个LRU链表上划分两个区域：young和old区域
都是将数据分为了冷数据和热数据，进行LRU算法
#### Linux如何避免预读失效带来的影响
- active_list活跃内存链表，保存最近被访问过的内存页
- inactive_list不活跃内存链表，保存很少被访问的内存页
预读页加入inactive_list头部，真正被访问的时候插入active_list，如果预读页一直没有被访问，就会从inactive_list移出，不会影响active_list的热点数据
#### Mysql如何避免预读失效带来的影响
young，old区域在链表中的占比是64：37，划分这两个区域后，预读页加入old头部，被真正访问时，再插入young头部
### 缓存污染怎么办
#### 什么是缓存污染
只要数据被访问一次，就将数据加入活跃LRU链表头部；批量读取数据时，将大量数据加入活跃LRU链表导致原本数据被淘汰，而这些大量数据很长一段时间又不会被访问，就污染了整个活跃LRU链表
#### 缓存污染会导致什么问题
淘汰大量热数据，导致热数据被访问时无法命中缓存，产生大量磁盘IO，降低系统性能
#### 怎么避免缓存污染造成影响
提高进入活跃区域的门槛，保证热数据不会被轻易替换
- Linux：内存页被第二次访问的时候，将页升级到activelist
- Innodb：内存页被第二次访问的时候，再进行在old区域停留时间的判断
	如果第二次访问的时间和第一次访问的时间在1s内就不会被升级到young，超过1s才会升级到young
# Linux虚拟内存管理
### 到底什么是虚拟内存地址
类似现实世界的“收货地址”，对应的是物理内存的真实地址
64位：全局页目录项+上层页目录项+中间页目录项+页表项+页内偏移
32位：页目录项+页表项+页内偏移
### 为什么要用虚拟内存访问内存
使进程之间的内存空间地址相互隔离，避免产生使用了一个物理地址造成冲突，同时多进程之间协同交给内核管理
### 进程虚拟内存空间
用户空间
- 存放程序二进制文件中的机器指令的代码段
- 存放二进制文件中定义的全局变量和静态变量的数据段和BSS段（未被初始化的会被初始化为0）
- 程序运行中动态申请内存的堆
- 存放动态链接库以及内存映射区域的文件映射与匿名映射区
- 存放函数调用过程中的局部变量和函数参数的栈
### Linux进程虚拟内存空间
#### 32位机器进程虚拟内存空间的分布
用户态虚拟内存空间3GB，内核态1GB
文件映射与匿名增长区、栈区的地址增方向是从高地址向低地址增长
#### 64位机器进程虚拟内存空间的分布
低128T表示用户态虚拟内存空间，高128T表示内核态虚拟内存空间，内核态和用户态之间形成一段地址空洞(anonical address)
### 进程虚拟内存空间的管理
进程描述符task_struct中有一个专门描述进程虚拟地址空间的内存描述符mm_struct，每个进程都有唯一的mm_struct。创建子线程时，调用和fork()在copy_process中创建task_struct，子进程在新创建出来后的虚拟内存空间和父进程是一模一样的
通过fork()函数创建出的子进程的虚拟内存空间以及相关页表相当于父进程的一个拷贝；而用vfork或者clone创建出的子线程会设置CLONE_VM，在copy_mm中将父进程值赋给子进程，相当于父进程和子进程是共享内存空间的了，是否共享地址空间几乎是进程和线程之间的本质区别
父进程和子进程的区别、进程和线程的区别、以及内核线程和用户线程的区别都是围绕着mm_struct展开的
#### 内核如何划分用户态和内核态虚拟内存空间
进程内存描述符的mm_struct中的task_size变量，定义了用户态和内核态地址之间的分界线
64位虚拟内存空间的布局和物理内存页page大小有关。默认PAGE_SIZE是4k
#### 内核如何布局进程虚拟内存空间
vm_area_struct描述了虚拟内存区域VMA，煤业vm_area_struct对应虚拟空间中的唯一虚拟内存空间VMA，vm_start指向起始最低地址，vm_start包含在区域内，vm_end指向结束地址最高地址，不包含这片区域
#### 定义虚拟内存区域的访问权限和行为规范
vm_page_prot和vm_flags是用来标记vm_area_struct结构表示的这块虚拟内存区域的访问权限和行为规范
#### 关联内存映射中的映射关系
anon_vma，vm_file，vm_pgoff分别和虚拟内存映射相关，虚拟内存区域可以映射到物理内存上，也可以映射到文件上，映射到物理内存上叫匿名映射，映射到文件中叫文件映射
使用mmap申请内存时，会在匿名映射区创建一块VMA内存区域，用struct anon_vma表示，vmfile属性就用来关联被映射的文件，vmpgoff表示映射进虚拟内存中的文件内容，在文件中的偏移，vmprivatedata用于储存vma中的私有数据
#### 针对虚拟内存区域的相关操作
vm_ops用来指向对于虚拟内存区域VMA相关操作的函数指针
- 指定虚拟内存被加入进程虚拟内存空间中时，open就会被调用
- 虚拟内存区域VMA被进程从虚拟内存空间中删除时，调用close
- 进程虚拟内存时，访问的页面不在物理内存中，可能是未分配物理内存也可能是被置换到硬盘中，就会触发缺页异常，调用fault
- 只读的页面要切换到可读时，调用page_mkwrite
#### 虚拟内存区域在内核中是如何被组织的
vm_area_struct的双向链表将VMA串连起来的，VMA双向链表有顺序，按照低地址到高地址增长方向排序；还有一种组织形式是红黑树，每个VMA都是红黑树的中一个节点，通过vm_area_struct中的vm_rb将自己连接到红黑树中
### 程序编译后的二进制文件如何映射到虚拟内存空间中
磁盘文件的段叫Section，内存的段叫做Segment，Section会在进程运行之前加载到内存中并映射到内存中的Segment，通常是多个Section映射到一个Segment；磁盘中的.text、.rodata等一些只可读的Section，会被映射到内存的一个只读可执行的Segment中（代码段），.data、.bss等可读写的Section会被加载到具有读写权限的Segment中（数据段，BSS），通过**load_elf_binary**函数将Section映射到虚拟内存空间
### 内核虚拟内存空间
内核态虚拟内存空间是所有进程共享的，不同进程进入内核态之后看到的虚拟内存空间都是一样的
#### 32位内核虚拟空间内存布局
通过TASK_SIZE将进程虚拟内存和内核虚拟内存隔开，PAGEOFFSET的值在32位系统下默认为0xC000 000
#### 直接映射区
1G大小的内核虚拟内存空间中有一个896M大小的区域，称为直接映射区（线性映射区），会映射到0-896M这块连续的物理内存上，映射关系是一比一；前1M被系统占用，后存放内核代码段、数据段、BSS段，fork创建的task_struct、mm_struct、vm_area_struct等进程相关的数据结构也存放在里面
进程创建完毕后，每个进程会被分配一个固定大小的内核栈，每个进程调用链都放在内核栈中，内核栈也是分配在直接映射区
	用户栈可以动态扩容，而内核栈溢出会覆盖相邻内存中的数据，破坏数据
直接映射区的前16M内存用来为DMA分配内存，称之为ZONE_DMA，用于DMA的内存必须从ZONE_DMA中分配，剩下的区域叫ZONE_NORMAL
#### ZONE_HIGHMEM高端内存
物理内存896M以上区域被划分为ZONE_HIGHMEM，高端内存，通过动态映射将3200M物理内存映射到128M内核虚拟内存中，先映射使用的部分，使用完毕解除映射，映射其他部分
内核虚拟空间中3G+896M在内核定义为high_memory，往上有一段8M的内存空洞，从high_memory到VMALLOC_START
#### VMALLOC动态映射区
和用户态malloc申请内存一样，动态映射区内核使用vmalloc进行内存分配，分配的内存在虚拟上是连续的，但物理内存不是连续的，通过页表建立联系
#### 永久映射区
PKMAP_BASE到FIXADDR_START称为永久映射区，允许建立与物理高端内存的长期映射关系，比如alloc_pages()在物理内存的高端内存申请到的物理内存页，通过kmap映射到永久映射区中
#### 固定映射区
FIXADDR_START到FIXADDR_TOP，固定映射区中虚拟内存地址可以自由映射到物理内存的高端地址上，但与动态映射区和永久映射区不同的是，固定映射区中虚拟地址是固定的，被映射的物理地址是可以改变的
在内核启动过程中，一些模块无法等待完整的内存管理模块初始化后再地址映射，内核就固定分配了一些虚拟地址，使用该地址的模块在初始化的时候，将固定分配的虚拟地址映射到指定的物理地址上去
#### 临时映射区
内核将用户空间缓冲区的数据拷贝到pagecache中，kmap_atomic将缓存页临时映射到内核空间的一段虚拟内存上，这段虚拟内存就是临时映射区，然后将用户缓冲区待写入数据通过这段虚拟地址拷贝到page cache相应缓存中，拷贝完成后调用kunmap_atomic将这段映射删掉
#### 64位操作系统
512M代码段-1T虚拟内存映射区-1T空洞-32T vmalloc-1T空洞-64T直接映射区-8T空洞
### 到底什么是物理内存地址
- SRAM静态RAM，用于L1L2L3cache
- DRAM动态RAM，主存
DRAM储存结构是二维矩阵，储存的元素称为超单元supercell，每个supercell大小为一个字节8bit，每个supercell都有一个坐标地址
#### DRAM芯片的访问
1. 储存控制器将行地址RAS = 2通过地址引脚发给DRAM芯片
2. DRAM芯片根据RAS=2将二维矩阵第二行全部内容拷贝到内部行缓冲区中
3. 储存控制器通过地址引脚发送CAS=2到DRAM芯片中
4. DRAM芯片从内部行缓冲区中根据CAS=2拷贝出第二列的supercell并通过数据引脚发给储存控制器
#### CPU如何读写主存
CPU与内存通过总线进行数据交互，传输的信号包括地址信号、数据信号、控制信号，控制信号可以同步事务，标识出当前正在被执行的事务信息，**总线传输的都是物理地址**；IO bridge将总线上的电子信号转化为储存总线上的电子信号
#### CPU从内存读取数据过程
CPU总线发起读事务
1. CPU将物理内存地址A放到总线上，IO bridge将信号传到储存总线上
2. 主存感受到地址信号并通过储存控制器将储存总线上的物理地址A读出来
3. 存储控制器通过物理地址A定位到具体存储模块，从DRAM芯片中取出对应数据X
4. 控制器将X放到数据总线上，IObridge将数据信号转换为系统总线上的数据信号
5. CPU感受到数据信号，将数据总系统总线上读取出来并拷贝到寄存器中
#### 如何根据物理内存地址从主存中读取数据
存储控制器将物理内存地址转换为DRAM芯片中supercell的坐标地址（RAS，CAS），将地址发给对应的存储器模块，随后存储器模块将RAS和CAS广播到存储器模块中的所有DRAM芯片，从0到7读取响应supercell
CPU以word size为单位从内存中读取数据，在64位处理器中word size为8字节
#### CPU向内存写入数据过程
1. CPU将要写入的内存物理地址A放入系统总线上
2. 通过IO bridge信号转换将物理地址A传输到存储总线上
3. 存储控制器感受到存储总线上的地址信号，将物理内存地址A从存储总线上读取出来，并等待数据到达
4. CPU将寄存器中的数据拷贝到系统总线上，通过IO bridge信号转换，将数据传递到存储总线上
5. 存储控制器感受到存储总线上的数据信号，将数据从存储总线上读取出来
6. 存储控制器通过内存地址A定位到具体的存储器模块，最后将数据写入存储器模块的8个DRAM芯片中

# Linux物理内存管理

### 从CPU角度看物理内存模型
#### FLATMEM平坦内存模型
内核将连续的物理空间内存分为一页一页的内存块struct page，由于地址是连续的，划分出的物理页也是连续的，且每页大小都是固定的，用一个数组来组织这些连续的物理内存页struct page，在数组中对应下标PFN，这种内存模型就叫FLATMEM
内核使用一个mem_map全局数组来组织所有划分出的物理页，对应下标就是PFN，平坦内存模型下，page_to_pfn&pfn_to_page计算逻辑就是基于mem_map数组进行偏移操作
	平坦内存模型只适合管理一整块连续内存，对于多块非连续内存会导致很大内存空间浪费，由于mem_map全局数组组织划分出的物理页page，对于物理内存存在大量不连续内存地址时，这些不连续区间就形成了内存空洞，如果为每个空洞分配struct page（40字节）将会占用大量内存空间
#### DISCONTIGMEM非连续内存模型
内核将物理内存从宏观上划分成了一个个节点node，每个node管理一块连续的物理内存，避免了内存空洞造成的内存浪费
内核中使用struct pglist_data用于管理连续物理内存的node节点，每个节点内部使用FLATMEM平坦内存模型来管理，每个node包含一个`struct page *node_mem_map`来组织管理
非连续内存模型下page_to_pfn&pfn_to_page计算多了一步定位page所在node的操作
- 通过acrh_pfn_to_nid根据物理页的PFN定位到物理页所在的node
- page_to_nid根据物理页结构struct page定义到page所在node
	其实每个node中的物理内存也不一定都是连续的
#### SPARSEMEM稀疏内存模型
对粒度更小的连续内存块进行精细管理，用于管理连续内存的单元称为section。物理页大小为4k情况下，section大小为128M，物理页大小为16k情况下，section大小为512M
内核中使用struct mem_section表示section，每个struct中有section_mem_map指针用于管理连续page数组；所有mem_section会被放在一个全局的数组中，且每个mem_section都可以在系统运行时改变offline/online，以便支持内存热插拔
- page_to_pfn转换中，首先通过page_to_pfn根据struct page结构定位到mem_strcut中具体的section结构，再通过section_mem_map定位到具体PFN
	struct page中有一个unsigned long flags属性，flags的高位bit中存着page所在mem_section数组中的索引，从而可以定位到所属section
- pfn_to_page中，首先通过__pfn_to_section根据PFN定位到mem_section中具体section结构，然后再通过PFN在section_mem_map数组中定位到具体物理页page
	PFN高位bit存储的是全局数组mem_section中的section索引，PFN的低位bit存储的是section_mem_map数组中具体物理页page的索引
#### 物理内存热插拔
- 物理热插拔阶段：物理上将内存硬件插入、拔出，涉及硬件和内核的支持
- 逻辑热插拔阶段：由内核中的内存管理子系统负责，涉及到如何动态的上线启用刚刚hotadd的内存和如何动态下线刚刚hotremove的内存
稀疏内存模型中，每个mem_section都可以在系统运行时改变offline\online状态，当offline时，内核会吧这部分内存隔离开，使不可再被使用，再把mem_section中已经分配的内存页迁移到其他mem_section上；
	并非所有物理页都可以迁移，迁移意味着物理内存地址变化，而内存热插拔应该对进程来说是透明的，这些迁移后的物理页映射的虚拟内存地址是不能变化的；这一点在用户空间是没有问题的，进程的用户空间访问内存都是根据页表查对应物理地址，但是内核态的直接映射区中虚拟地址都是直接映射到物理内存上，虚拟地址会随着物理地址变化而变化，无法被轻易迁移的，内核通过分类物理页是否可迁移，分为不可迁移页、可回收页、可迁移页，然后在可能被拔出的内存中只分配可迁移的内存页，使不可迁移的页就不会被包含在可能会拔出的内存中
### CPU角度看物理内存架构
#### 一致性内存访问UMA架构
多个CPU位于总线的一侧，所有内存组成一大片内存位于总线的另一侧，所有CPU访问都要过总线，且距离一致，在UMA架构下所有CPU访问内存速度都是一样的
CPU个数越多，就会造成性能瓶颈
- 总线带宽压力越来越大，随着CPU增多导致每个CPU可用带宽会减少
- 总线长度增加，进而增加访问延迟
#### 非一致性内存访问NUMA架构
内存被分为一个一个内存节点（NUMA节点），每个CPU都有自己的本地内存节点，访问自己的节点不需要通过总线，当本地内存不足时，就会跨节点去访问其他内存；访问自己节点速度是最快的，访问其他节点会慢很多，导致CPU访问内存速度不一致，所以叫非一致性内存访问架构
CPU和CPU之间通过QPI点对点完成互联，CPU本地内存不足时，通过QPI访问远程NUMA节点的内存控制器分配内存
#### NUMA的内存分配策略
- MPOL_BIND 必须在绑定的内存节点进行内存分配，如果内存不足则进行swap
- MPOL_INTERLEAVE 本地节点和远程节点均可允许分配内存
- MPOL_PREFERRED 优先在指定节点分配内存
- MPOL_LOCAL（默认） 优先在本地节点分配，当本地节点内存不足时，可以在远程节点分配内存
### 内存如何管理NUMA节点
在NUMA架构下只有DISCONTIGMEM非连续内存模型和SPARSEMEM稀疏内存模型是可用的，UMA下前面介绍的三种内存模型都可用
无论是UMA还是NUMA架构在内核中都是使用相同数据结构来组织管理的，在内核的内存管理模式中会把UMA当作只有一个NUMA节点
	NUMA节点中可能会有多个CPU，这些CPU都是物理CPU
#### 内核如何统一组织NUMA节点
strcut pglist_data来描述NUMA节点，内核2.4之前，使用pgdat_list单链表将NUMA节点串联起来；每个NUMA节点的数据结构pglist_data有一个next指针，用于将节点串联起来形成pgdat_list单链表，2.4之后使用一个大小为MAX_NUMNODES，类型为struct pglist_data的全局数组node_data来管理所有NUMA节点
#### pglist_data结构
node_mem_map包含了NUMA节点内所有的物理内存页
node_start_pfn指向NUMA节点内第一个物理页的PFN，所有NUMA节点都是依次编号的，每个PFN都是全局唯一的
node_present_pages用于统计NUMA节点内所有真正可用的物理页面数量
#### NUMA节点物理内存区域的划分
1. ZONE_DMA：用于无法对全部物理内存进行寻址的硬件设备，进行DMA时的内存分配
2. ZONE_DMA32：提供给32位设备进行DMA操作
3. ZONE_NORMAL：这个区域的物理页都可以直接映射到内核中的虚拟内存，由于是线性映射，内存都可以直接访问
4. ZONE_HIGHMEM：内存不能直接访问，需要动态映射进内核虚拟内存空间中，只在32位系统中存在
ZONE_DEVICE为支持热插拔设备而分配的易失性内存
ZONE_MOVABLE是内核定义的一个虚拟内存区域，是全部可迁移的，为了防止内存碎片和支持内存的热插拔；可以通过迁移页面避免内存碎片的问题，这就是ZONE_MOVABLE被划分的目的
不是每个节点都有所有物理内存，不同节点之间包含的物理内存区域个数是不一样的
#### NUMA节点中的内存规整与回收
内核会为每个NUMA节点分配一个kswapd进程回收不常使用的页面，还会为每个节点分配一个kcompactd进程用于内存的规整避免内存碎片
kswaped_wait用于kswaped进程周期性回收页面所需要用到的等待队列
task_struct kcompactd用于指向内核为NUMA节点分配的kcompactd进程
kcompactd_wait用于kcompactd进程周期性规整内存时使用到的等待队列
#### NUMA节点的node_states
NUMA节点多于一个，内核就会维护一个node_states用于维护各个NUMA节点的状态值
### 内核如何管理NUMA节点中的物理内存区域
struct zone中频繁访问的字段信息归为4个部分，通过ZONE_PADDING来分割，通过填充字节将四个部分填充到不同的CPU高速缓存行中；多核处理器中，多个CPU会并发访问struct zone，内核使用一把spinlock_t lock自旋锁来防止并发错误以及不一致
#### 物理内存区域中的预留内存
nr_reserved_highatomic表示该内存区域预留内存的大小，范围从128-65538KB之间
lowmem_reserve数组规定每个内存区域必须为自己保存的物理页数量，为了防止更高位内存区域对自己的内存空间进行过多的侵占挤压
	高位内存区域中物理内存不够用了，内核就会占用挤压其他内存区域中的物理内存满足内存分配的需求
- ZONE_DMA防止被ZONE_NORMAL挤压，预留64内存页
- ZONE_DMA防止被ZONE_HIGHMEM挤压，预留320内存页
- ZONE_NORMAL预留2048内存页
#### 物理内存区域中的水位线
内存不足时
- 产生OOM，将OOM优先级最高的进程干掉
- 内存回收
- 内存规整
swappiness内核选项控制回收文件页和匿名页倾向，数值越大越倾向于回收匿名页
- 内存剩余容量高于_watermark(WMARK_HIGH)时，说明物理内存容量充足
- 在_watermark(WMARK_LOW)与_watermark(WMARK_HIGH)之间是，说明内存有一定消耗但是可以接收，能满足进程的内存分配需求
- 剩余容量在_watermark(WMARK_LOW)与_watermark(WMARK_MIN)之间是，说明内存分配存在压力，给进程分配完内存后会唤醒kswapd开始内存回收，直到高于_watermark(WMARK_HIGH)
- 当剩余内存容量低于_watermark(WMARK_MIN)时，说明此时内存容量已经非常危险了，如果进程这时请求内存分配，内核就会进行直接内存回收，请求进程会同步阻塞等待，直到内存回收完毕
#### 水位线的计算
WMARK_MIN、WMARK_LOW、WMARK_HIGH通过min_free_kbytes进行计算，LOW的值是MIN的1.25x，HIGH是LOW的1.5x
#### min_free_kbytes的计算逻辑
首先计算出当前NUMA节点中所有低位内存区域的内存总容量之和即lowmem_kbytes的值，然后init_per_zone_wmark_min会对lowmem_kbytes* 16进行开方得到new_min_free_kbytes，如果大于内核参数值min_free_kbytes就取new，随后内核根据这个值计算出三条水位线，然后setup_per_zone_lowmem_reserve计算内存区域的预留内存大小
#### setup_per_zone_wmarks计算水位线
for_each_zone循环内依次遍历NUMA节点中的所有内存区域zone，计算每个内存区域zone的内存水仙，do_div计算WMARK_MIN，通过每个zone之间的比例，根据比例去从min_free_kbytes中划分出对应zone的WMARK_MIN水位线
#### watermark_scale_factor调整水位线的间距
保证kswapd活动范围大一些，内核就能时刻进行内存回收使得剩余内存容量较长时间保持在WMARK_HIGH水位线之上，动态计算水位线间距，使kswapd有时间回收足够的内存，解决直接内存回收导致的性能抖动问题
#### 物理内存区域中的冷热页
//waiting
# 进程、线程
### 进程
CPU执行编译后的二进制可执行程序，装载到内存中，这个运行中的程序就叫进程
当进程需要从硬盘读取数据时，CPU就不需要阻塞等待数据的返回去执行别的进程，当数据返回时，CPU会收到中断再运行这个进程
单核CPU在某一瞬间只能运行一个进程，但一秒内可能运行多个进程，这样就产生**并行**的错觉，其实是**并发**
#### 进程的状态
- 运行状态：该进程时刻占用CPU
- 就绪状态：可运行，由于其他进程处于运行状态而暂时停止运行
- 阻塞状态：该进程正在等待某事件发生（如等待输入/输出完成）而暂时停止运行，这时CPU给他控制权也无法运行
两个基本状态
- 创建状态：进程正在被创建时的状态
- 结束状态：进程正在从系统中消失时的状态
状态的跃迁
- NULL->创建状态：新进程被创建
- 创建状态->就绪状态：进程被创建并初始化后，等待就绪准备运行
- 就绪态->运行状态：就绪态的进程被操作系统的进程调度器选中后，分配给CPU正式运行
- 运行状态->结束状态：进程运行完毕或出错时，被操作系统作结束状态处理
- 运行状态->就绪状态：运行过程中分配的时间片用完，操作系统就把该进程变为就绪态，从就绪态中选择另一个进程运行
- 运行状态->阻塞态：当进程请求某个时间且必须等待时
- 阻塞态->就绪态：进程等待的时间完成后，从阻塞态进入就绪态
虚拟内存管理时，通常把阻塞状态的进程的物理内存换出到硬盘，需要再次运行时，再换入到物理内存，**挂起状态**描述进程没有实际占用物理内存空间
- 阻塞挂起状态：进程在外存并等待某个时间的出现
- 就绪挂起状态：进程在外存，但只要进入内存就立刻运行
导致进程挂起原因不止因为进程所使用的内存空间不在物理内存空间
- 通过sleep让进程间歇性挂起，原理是设置定时器到期后唤醒进程
- 用户希望一个进程的挂起
#### 进程的控制结构
OS中用**进程控制块**描述进程，是进程存在的唯一标识，具体包含
**进程描述信息**：
- 进程标识符：每个进程有且唯一的标识符
- 用户标识符：进程归属的用户，主要为共享和保护服务
**进程控制和管理信息**：
- 进程当前状态，如new、ready、running、waiting
- 进程优先级：抢占CPU时优先级
**资源分配清单**：
- 有关内存地址或虚拟地址空间的信息，所打开文件的列表和所使用的IO设备信息
**CPU相关信息**：
- 各个寄存器的值，当进程被切换时，CPU的状态信息都会被保存在相应的PCB中，以便进程断点继续执行
PCB通过链表进行组装，把有相同状态的进程连在一起，组成各种队列
- 把所有就绪状态进程连在一起，**就绪队列**
- 把所有因各种原因处在等待状态的进程连在一起，**阻塞队列**
- 对于单核CPU运行队列只有一个指针，因为单核CPU某个时刻只能运行一个程序
#### 进程的控制
**创建进程**
- 申请一个空白的PCB，并填写一些控制和管理进程的信息，如进程唯一标识符
- 为该进程分配运行时必须资源，比如内存资源
- 将PCB插入到就绪队列，等待被调度运行
**终止运行**
有三种终止方式：正常结束、异常结束、外界干预（kill）
子进程被终止时，在父进程继承的资源应当还给父进程；当父进程被终止时，子进程成为孤儿进程，被一号进程收养并对其完成状态收集工作
终止进程过程：
- 查找需要终止的进程的PCB
- 如果出于执行状态，立刻终止进程执行，将CPU资源分给其他进程
- 还有子进程，则将子进程交给一号进程接管
- 将进程所有的全部资源还给操作系统
- 将其从PCB所在队列中删除
**阻塞进程**
- 找到要被阻塞进程标识号对应的PCB
- 如果为运行状态，保护现场将其状态转为阻塞状态，停止运行
- 将该PCB插入阻塞队列中
**唤醒进程**
- 将该事件的阻塞队列中找到对应进程的PCB
- 将其从阻塞队列中移出，状态设置为就绪状态
- 将该PCB插入就绪队列中，等待调度程序调度
#### 进程的上下文切换
CPU通过寄存器和程序计数器实现上下文切换
进程上下文切换包括虚拟内存、栈、全局变量等用户空间的数据，还包括了内核堆栈、寄存器等内核空间的资源。通常会把交换的信息保存在进程的PCB中，需要运行另一个进程时，从这个进程PCB取出上下文，恢复到CPU中
上下文切换时场景：
- 保证各个进程公平调度，CPU将时间分为时间片分配给各个进程，当进程时间片耗尽时，进程就从运行状态切换到就绪状态
- 进程在资源不足时，等待资源满足后才可以运行，这个时候进程也会被挂起，由系统调度其他进程运行
- 进程通过sleep方法将自己主动挂起
- 有更高优先级进程运行时，当前进程会被挂起
- 发生硬件中断时，CPU进程会被挂起，执行内核中的中断服务程序 
### 线程
线程是进程当中的一条执行流程
- 一个进程中可以存在多个线程
- 各个线程之间可以并发执行
- 各个线程之间可以共享地址空间和文件等资源
- ***缺点***：当进程中一个线程奔溃时，会导致其所属的所有线程奔溃
#### 线程和进程的比较
- 进程是资源分配的单位，线程是CPU调度的单位
- 进程拥有一个完整的资源平台，线程只共享必不可少的资源，如寄存器和栈
- 线程同样具有就绪、阻塞、执行三种基本状态
- 线程能减少并发执行的时间和空间开销
线程相比进程能减少开销
- 创建时间快，只需要共享资源管理信息
- 同一个进程内的线程切换比进程切换快，因为线程有相同的地址空间，不需要切换页表
- 同一进程内的线程共享内存和文件数据，线程之间传输数据效率高
#### 线程的上下文切换
进程和线程最大的区别：线程是调度的基本单位，而进程则是资源拥有的基本单位
操作系统的任务调度，实际上调度对象是线程，进程只是给线程提供了虚拟内存、全局变量等资源
- 当进程只有一个线程时，可以认为进程就等于线程
- 当进程拥有多个线程时，这些线程会共享相同的虚拟内存和全局变量等资源，在上下文切换的时候是不需要修改的
线程有自己的私有数据，比如栈和寄存器，上下文切换的时候也需要保存
线程上下文切换需要看是否处于同一个进程
- 不属于同一个进程，则切换的过程就和进程上下文切换一样
- 属于同一个进程，因为虚拟内存共享，切换时资源是不动的，只需要切换线程的私有数据、寄存器等不共享的数据
#### 线程的实现 
- 用户线程：在用户空间实现的线程，不是由内核管理的线程，用户态的线程库来完成线程的管理
- 内核线程：内核中实现的线程，内核进行管理
- 轻量级线程：内核中来支持用户线程
内核线程和用户线程也有**一对一、一对多、多对多**的关系
用户现场是基于内核态的线程管理库实现的，线程控制块TCB在库里实现，OS只能看到整个进程的PCB，用户线程的线程管理和调度，OS是**不直接参与**的，由用户级线程库函数完成线程管理
用户线程的优点：
- 每个进程都有独立的TCB列表，用来记录各个线程状态，TCB由用户级线程库函数维护，可用于不支持线程技术的操作系统
- 用户线程切换由库函数完成，无需用户态和内核态的切换
缺点
- OS不操作线程调度，如果一个线程发起了系统调用而阻塞，那进程包含的用户线程都不能执行了
- 线程开始运行后，除非主动交出CPU使用权，否则所在进程中的其他线程无法运行，因为用户态现场没有权限打断正在运行的线程，只有操作系统有，但是操作系统不参与用户线程管理
- 时间片分配给进程，与进程比，多线程执行时，每个线程得到的时间片少，执行会比较慢
**内核线程**由OS管理，对应TCB在操作系统中，这样线程的创建、终止和管理由操作系统负责
内核线程的优点
- 如果某个内核线程发起系统调用而被阻塞，不会影响其他内核线程的运行
- 分配给线程，多线程的进程获得更多CPU运行时间
内核线程的缺点
- 支持内核线程的操作系统中，由内核来维护进程和线程的上下文信息，如PCB和TCB
- 线程的创建、终止和切换都是通过系统调用的方式来进行的，对于系统来说内核开销比较大
	轻量级线程是内核支持的用户线程，一个进程可有多个LWP，每个LWP和内核线程一对一映射，由内核管理和普通进程一样被调度；和普通进程的区别是只有一个最小执行上下文和调度所需的统计信息，有1:1/1:N/N:N多个和内核对应的模式
### 调度
#### 调度时机
当进程状态发生变化时，就会触发一次调度，调度根据如何处理时钟中断，可以分为两类调度算法
- 非抢占式调度算法，挑选一个进程，让该进程运行到被阻塞，或直到该进程退出，才会调用另外一个进程，也就是说不会处理时钟中断这个事情
- 抢占式调度算法，挑选一个进程，让该进程只允许某段时间，如果在该时段结束时，该进程仍然在运行，则会挂起，调度程序从就绪队列中选择另一个进程。这种调度需要在时钟间隔的末端发生**时钟中断**，以便将CPU返回给调度程序调度，也就是**时间片机制**
#### 调度原则
原则一：为了提高CPU利用率，发送IO时间使CPU空闲情况下，调度程序需要从就绪队列中选择一个进程来运行
原则二：提高程序吞吐率，调度程序需要权衡长任务和短任务进程的任务完成数量
原则三：如果进程的等待时间长而运行时间短，那周转时间（运行时间+等待时间）就很长，调度程序应当避免这种情况发生
原则四：就绪队列中的等待时间也是调度程序所需要考虑的原则
原则五：对于交互式比较强的应用，响应时间也是调度程序需要考虑的原则
- CPU利用率：调度程序应当保证CPU始终是匆忙，提高CPU利用率
- 系统吞吐量：长作业进程会占用较长时间CPU资源，会降低吞吐量，短作业进程会提高系统吞吐量
- 周转时间：一个进程周转时间应越短越好
- 等待时间：进程处于就绪队列的状态，越短越好
- 响应时间：用户提交请求到系统第一次产生响应的时间，交互式系统中响应时间是衡量调度算法好坏的主要标准
#### 调度算法
- 先来先服务调度算法FCS：非抢占式，每次从就绪队列选择最先进入队列的进程，然后一直运行，直到退出或被阻塞，才会选择第一个进程接着运行。适合CPU繁忙型作业
- 最短作业优先调度算法SJF：优先选择运行时间最短的进程来运行，对长作业不利
- 高响应比优先调度算法HRRN：每次进行进程调度时，先计算响应比优先值，然后把响应比优先值最高的进程投入运行。理想型算法，现实中实现不了
- 时间片轮转调度算法RR：每个进程分配一个时间段，称为时间片，允许该进程在该时间段内运行。一般设置为20~50ms合理
- 最高优先级算法HPF：分为静态优先级和动态优先级（随着时间推移增加等待进程的优先级），有非抢占式和抢占式处理优先级高的方法，会导致低优先级进程永不运行
- 多级反馈队列调度算法MFQ：是时间片轮转算法和最高优先级算法的综合和发展；多个队列优先级从高到低，优先级越高时间片越短，新的进程会被放进第一级队列队尾，按照FCS等待被调度，如果第一级队列规定时间没允许完成，转入第二队列队尾，当高优先级队列为空才调度低优先级队列进程允许，有新进程进入较高优先级队列中时，停止当前运行进程并移入原队列末尾，让高优先级队列进程运行
# 进程间通信方式
### 管道
Linux命令中的`|`是管道，将前一个命令的输出作为后一个命令的输入，管道传输数据是单向的，相互通信需要创建两个管道，同时以`|`表示的管道称为匿名管道。还有一种管道类型为命名管道，也被称为`FIFO`
所谓的管道就是内核中的一段缓存，从管道一段写入的数据，实际上缓存在内核中，另一端读取，就是从内核中读取这段数据，管道传输的数据是无格式的流且大小受限
管道的通信方式效率低，不适合进程间频繁的交换数据
### 消息队列
消息队列适合进程间频繁的交换数据
消息队列是保存在内核中的消息链表，发送数据时，会分成一个个独立的数据单元（消息体），是用户自定义的数据类型，是固定大小的储存块，进程从消息队列中读取了消息体，内核就会把这个消息体删除；消息队列的生命周期随内核，如果没有释放或者关闭操作系统，消息队列会一直存在，而匿名管道是随着进程创建和销毁
消息队列不适合大数据的传输，在内核中每个消息体会有最大长度的限制；消息队列通信通信不及时，存在用户态和内核态之间的数据拷贝开销
### 共享内存
共享内存的机制，是拿出一块虚拟地址空间来，映射到相同的物理内存中，无需进行拷贝
但是存在多个进程同时修改一片内存，导致冲突，先写的内容会被覆盖
### 信号量
防止多进程竞争共享资源造成数据错乱，**信号量**实现了保护机制
信号量是一个整型的计数器，用于实现进程间的互斥与同步，而不是用于缓存进程间通信的数据，控制信号量的方式有两种原子操作：
- P操作，将信号量-1，相减后如果信号量<0表示资源已经被占用，进程阻塞等待；如果≥0则表明还有资源可以操作，进程继续执行
- V操作：将信号量+1，相加后如果信号量小于等于0则表明当前有阻塞中的进程，将该进程唤醒运行；如果>0则表明当前没有阻塞中的进程
P操作在进入共享资源之前，V操作在离开共享资源之后
信号初始化为1，代表是互斥信号量，保证共享内存只有一个进程在访问；初始化为0，代表是同步信号量，保证进程间的执行顺序
### 信号
对于异常状态下的工作模式，需要**信号**的方式通知进程
信号是进程间通信机制下**唯一的异步通信机制**，可以在任意时间发生信号给某一进程，有三种用户进程对信号的处理方式：
1. 执行默认操作：Linux对信号规定了默认操作，执行默认操作
2. 捕捉信号：为信号定义一个信号处理函数，当信号发生时，就执行相应的信号处理函数
3. 忽略信号：不希望处理某些信号的时候，就忽略该信号，不做任何处理。有两个信号是应用进程无法捕捉和忽略的，即`SIGKILL`和`SEGSTOP`，用于在任何时候中断或结束某一进程
### Socket
跨网络和不同主机上的进程进行通信使用Socket通信，还可以在同主机进程间通信
[TCP创建Socket](./图解网络#TCP三次/四次握手###针对TCP如何socket编程)
**UDP创建Socket**：创建socket后bind IP和Port，调用sendto和recvform即可。只要有一个socket，多台机器就可以任意通信，每个UDP socket都需要bind，每次调用时都需要传入目标主机的IP和Port
**本地Socket**：本地socket的编程接口和IPV4，IPV6套接字接口是一样的，支持字节流和数据报两种协议；实现效率高于IPV4IPV6的字节流、数据报socket实现；本地socket在bind时候，无需bind IP和接口，而是绑定一个本地文件
# 多线程冲突了怎么办
### 竞争和协作
#### 互斥的概念
**竞争条件**，当多线程相互竞争操作共享变量时，在执行过程中发生了上下文切换，得到了错误的结果。事实上，每次运行都可能得到不同的结果，输出的结果存在**不确定性**
多线程执行操作共享变量的这段代码可能会导致竞争状态，因此将这段代码称为**临界区**，它是访问共享资源的代码片段，一定不能给多线程同时执行；我们希望这段代码是**互斥**的，也就是保证一个线程在临界区执行时，其他线程应该被阻止进入临界区
#### 同步的概念
并发进程/线程在一些关键点上可能需要相互等待与互通消息，这种相互制约的等待和互通信息为进程/线程同步，比如“操作A需要在操作B之前执行”
### 互斥和同步的实现和使用
操作系统提供了两种方法保证进程间协作：
- 锁：加锁、解锁操作
- 信号量：P、V操作
#### 锁
任何想进入临界区的线程，必须先执行加锁操作，加锁通过则可进入临界区；完成对临界资源的访问后再执行解锁操作，释放资源
根据锁实现的不同分为**忙等待锁**和**无忙等待锁**
***原子操作指令——测试和置位指令：**
```C
int TestAndSet (int *old_ptr, int new){
 int old = *old_ptr
 *old_ptr = new
 return old
}
```

代码是原子执行的，既可以测试旧值又可以测试新值，可以使用TestAndSet实现忙等待锁
当获取不到锁，线程会一直while循环不做任何事情，就被称为**忙等待锁**，也被称为**自旋锁spin lock**
	一直自旋利用CPU周期直到锁可用，在单核CPU上需要抢占式的调度器 ，否则在单CPU上无法使用，因为一个自旋的线程永远不会放弃CPU
没获取到锁的时候，就把当前线程放进锁的等待队列中，然后执行调度程序，把CPU让给其他线程执行，这就是**无等待锁**
#### 信号量
操作系统提供的一种协调共享资源访问的方法，通常信号量表示**资源的数量**，对应的是一个sem整型变量，还有**两个原子操作的系统调用来控制信号量**：
- P：将 sem - 1，相减后＜0则进程进入阻塞，否则继续，表明P可能会阻塞
- V：将sem + 1，相加后如果sem≤0，唤醒一个等待中的线程，表明V不会阻塞
P，V必须成对出现，使用信号量实现临界区的互斥访问：
- 为每个资源设置信号量初值为1，只要把进入临界区的操作置于PV之间，可实现进程/线程互斥
- 设置信号量初始为0，可实现事件同步
#### 生产者-消费者问题
- 生产者在生成数据后，放在一个缓冲区中
- 消费者从缓冲区中取出数据处理
- 任何时刻，只能有一个生产者或消费者可以访问缓冲区
可以得出
- 任何时刻只能有一个线程操作缓冲区，说明操作缓冲区是临界代码，需要互斥
- 缓冲区空时，消费者必须等待生产者生成数据；缓冲区满时，生产者必须等待消费者取出数据，说明生产者和消费者**需要同步**
需要三个信号量：
- 互斥信号量`mutex`：用于互斥访问缓冲区，初始化值为1
- 资源信号量`fullbuffers`：用于消费者询问缓冲区是否有数据，有数据则读取数据，初始化为0
- 资源信号量`emptybuffers`：用于生产者询问缓冲区是否有空位，有空位则生成数据，初始化为n
### 经典同步问题
#### 哲学家就餐问题
*方案一*：信号量，P代表拿起叉子，V代表放下，没有叉子就等待其他人放下叉子
**极端情况**下每个人都拿起左边叉子，桌面上就没有叉子，又没有人能拿到右边叉子，就被阻塞发生了死锁
*方案二*：拿叉子前增加互斥信号量，只要有一个人进入了临界区（拿叉子操作），其他所有人都不能动，直到用完叉子才能让下一个人拿。每次只有一个人能操作，效率不高
*方案三*：让偶数编号先拿左边再拿右边，奇数先拿右边再拿左边
*方案四*：用数组state记录每个人的三个状态（进餐、思考、饥饿等待拿叉子），只有两个邻居都不在进餐，才可以进入进餐
#### 读者-写者问题
读者只会读取问题，写者既可以读也可以写
- 读-读允许：同一时刻允许多个读者相互读
- 读-写互斥：没有写者时刻读者才能读，没有读者时写者才能写
- 写-写互斥：没有其他写者时，写者才能写
*方案一*：
- 信号量`wMutex`：控制写操作的互斥信号量，初始为1
- 读者计数`rCount`：正在读操作的读者个数，初始为0
- 信号量`rCountMutex`：控制对rCount读者计数器的互斥修改，初始为1
**读者有限**，只要有读者在读，后续读者都可以直接进入，如果读者持续不断进入，则写者会处于饥饿状态
*方案二*：
- 只要有写者准备写入，写者应尽快执行写操作，后来读者必须阻塞
- 有写者不断写入，则读者持续饥饿
新增如下变量：
- 信号量`rMutex`：控制读者进入的互斥量，初始为1
- 信号量`wDataMutex`：控制写者写操作的互斥信号量，初始为1
- 写者计数`wCount`：记录写者数量，初始为0
- 信号量`wCountMutex`：控制wCount互斥修改，初始为1
*方案三*：
公平策略 
- 优先级相同
- 读者、写者互斥
- 只有一个写者访问临界区
- 可以有多个读者同时访问临界资源
# 怎么避免死锁